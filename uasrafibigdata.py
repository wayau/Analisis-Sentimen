# -*- coding: utf-8 -*-
"""UASrafiBigdata.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HdOQp8YG5fYpFB6MUNOjev9Dy9DuWfxz
"""

!pip install tweet-preprocessor
!pip install textblob
!pip install wordcloud
!pip install nltk
#untuk mendowload library tweet-preprocessor,textblob,wordcloud,nltk

import nltk
#untuk memanggil libaray nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer
#melakukan sentiment vader atau sentiment analsisi intensitas
nltk.download("vader_lexicon")
#ini untuk mendowload vader lecivon
import pandas as pd
data = pd.read_csv('data_kotor.csv', encoding='ISO-8859-1')
#ini nama data yang diupload
data = data.dropna()
print(data.head())
#mengeprint data yang telah di upload

sentiments = SentimentIntensityAnalyzer()
data["Positive"] = [sentiments.polarity_scores(i)["pos"] for i in data ["terjemahan"]]
data["Negative"] = [sentiments.polarity_scores(i)["neg"] for i in data ["terjemahan"]]
data["Neutral"] = [sentiments.polarity_scores(i)["neu"] for i in data ["terjemahan"]]
data['Compound'] = [sentiments.polarity_scores(i)["compound"] for i in data ["terjemahan"]]
data.head()

import preprocessor as p
from textblob import TextBlob
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize

data = pd.read_csv('data_kotor.csv')

data.isnull().sum()

def preprocessing_data(x):
    return p.clean(x)

def tokenize_data(x):
    return p.tokenize(x)

data['tweet_clean'] = data['terjemahan'].apply(preprocessing_data)
data['tweet_clean'] = data['tweet_clean'].apply(tokenize_data)
data = data.drop_duplicates()

import string
string.punctuation
def remove_punctuation(text):
    punctuationfree="".join([i for i in text if i not in string.punctuation])
    return punctuationfree
data['tweet_clean']= data['tweet_clean'].apply(lambda x:remove_punctuation(x))
data['tweet_clean']= data['tweet_clean'].apply(lambda x: x.lower())
data = data.drop_duplicates()

data.head(20)

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np

data.to_csv('data_bersih.csv', index=False)

ps = PorterStemmer()

def stemming_data(x):
    return ps.stem(x)

data['tweet_clean'] = data['tweet_clean'].apply(stemming_data)
data['tweet_clean'] = data['tweet_clean'].apply(tokenize_data)
data = data.drop_duplicates()

data_tweet = list(data['tweet_clean'])
polaritas = 0

status = []
total_positif = total_negatif = total_netral = total = 0

for i, tweet in enumerate(data_tweet):
    analysis = TextBlob(tweet)
    polaritas += analysis.polarity

    if analysis.sentiment.polarity > 0.0:
        total_positif += 1
        status.append('Positif')
    elif analysis.sentiment.polarity == 0.0:
        total_netral += 1
        status.append('Netral')
    else:
        total_negatif += 1
        status.append('Negatif')

    total += 1

print(f'Hasil Analisis Data:\nPositif = {total_positif}\nNetral = {total_netral}\nNegatif = {total_negatif}')
print(f'\nTotal Data : {total}')

status = pd.DataFrame({'klasifikasi': status})
data['klasifikasi'] = status
data.tail()

from wordcloud import WordCloud, STOPWORDS

def plot_cloud(wordcloud):
    plt.figure(figsize=(12, 8))
    plt.imshow(wordcloud)
    plt.axis("off");

all_words = ' '.join([tweets for tweets in data['tweet_clean']])
wordcloud = WordCloud(width = 3000, height = 2000, random_state=3, background_color='white', colormap='Set2', collocations=False, stopwords = STOPWORDS).generate(all_words)
plot_cloud(wordcloud)

all_words = ' '.join([tweets for tweets in data['terjemahan']])
wordcloud = WordCloud(width = 3000, height = 2000, random_state=3, background_color='white', colormap='Set2', collocations=False, stopwords = STOPWORDS).generate(all_words)
plot_cloud(wordcloud)

def show_pie(label, data, legend_title) :
    fig, ax = plt.subplots(figsize=(8, 10), subplot_kw=dict(aspect='equal'))

    labels = [x.split()[-1] for x in label]

    def func(pct, allvals):
        absolute = int(pct/100.*np.sum(allvals))
        return "{:.1f}% ({:d})".format(pct, absolute)

    wedges, texts, autotexts = ax.pie(data, autopct=lambda pct: func(pct, data),
                                      textprops=dict(color="w"))

    ax.legend(wedges, labels,
              title= legend_title,
              loc="center left",
              bbox_to_anchor=(1, 0, 0.5, 1))

    plt.setp(autotexts, size=10, weight="bold")
    plt.show()

label = ['Positif', 'Negatif', 'Netral']
count_data = [total_positif+1, total_negatif+1, total_netral]

show_pie(label, count_data, "Status")

"""Kesimpulan:
Masyarakat menanggapi secara netral terhadap kasus pneumolia atau coronabaru
"""

from google.colab import files

# Simpan DataFrame Anda ke file CSV
data.to_csv('hasil_data.csv')

# Mendownload file ke komputer lokal Anda
files.download('hasil_data.csv')